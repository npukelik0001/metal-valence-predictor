# -*- coding: utf-8 -*-
"""Spotify[Emotional Valence].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Cz6oeLNoFhstUnvOAs65f9pVwtxvm9S

Smaller artists and independent labels in the metal scene often struggle to anticipate which songs will perform well on platforms like Spotify. This makes it difficult to prioritize which tracks to promote.

In this project, I use regression analysis to predict a song’s emotional valence based on its audio features (examples: tempo, energy, loudness).

The focus is on subgenres like metalcore, death-metal, black-metal, heavy-metal, and metal. The goal is to create a simple, data-driven tool that helps teams make informed release and promotion decisions.
"""

# Importing libraries and loading the data set
# Load all required packages for data handling, visualization, and modeling
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import PolynomialFeatures
import warnings
warnings.filterwarnings('ignore')


import os
from dotenv import load_dotenv

# Load variables from .env file
load_dotenv()

# Get the values from the environment
kaggle_username = os.getenv('KAGGLE_USERNAME')
kaggle_key = os.getenv('KAGGLE_KEY')

# Set them for the Kaggle API to use
os.environ['KAGGLE_USERNAME'] = kaggle_username
os.environ['KAGGLE_KEY'] = kaggle_key

# Install Kaggle (remove # if not done and download it)
#!pip install -q kaggle

# Download the dataset
!kaggle datasets download -d maharshipandya/-spotify-tracks-dataset --force

import zipfile

with zipfile.ZipFile("-spotify-tracks-dataset.zip", 'r') as zip_ref:
    zip_ref.extractall(".")

# Load dataset into dataframe
df = pd.read_csv("dataset.csv")

# Preview the dataset
print("DATASET OVERVIEW")
print(f"Dataset shape: {df.shape}")
print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")
df.head()

# Looking at first 5 rows
print("FIRST 5 ROWS")
df.head()

# Looking at column names and data types
print("COLUMN NAMES AND DATA TYPES")
df.info()

# Summary statistics
df.describe()

# List of columns in the dataset
print("COLUMN NAMES")
list(df.columns)

"""Data Cleaning - I used a large Spotify dataset and started by removing any duplicate and missing rows to clean it up. Then I filtered it down to only include songs from metal genres I’m focusing on, genres like black-metal, metalcore, death-metal, heavy-metal, and metal. I dropped columns that weren’t useful for prediction, like track_id, mode, and explicit."""

# Check dataframe for duplicated rows
duplicate_rows = df.duplicated().sum()
duplicate_rows

#Check dataframe for null/missing values
print("MISSING VALUES ANALYSIS")
missing_data = df.isnull().sum()
missing_percent = (missing_data / len(df)) * 100
missing_summary = pd.DataFrame({'Missing Data': missing_data, 'Missing %': missing_percent})
missing_summary

# Remove missing/null data from the dataset
df_cleaned = df.dropna()
df_cleaned.isnull().sum()
print("Number of rows left after cleaning:", df_cleaned.shape[0])
df_cleaned_percent = (df_cleaned.shape[0] / df.shape[0]) * 100
print(f"Percentage of rows left after cleaning: {df_cleaned_percent:.4f}%")

#Removing uneccesary columns
print("COLUMN NAMES AFTER CLEANUP")
# Defining all columns to drop in one place
drop_columns = ['Unnamed: 0', 'track_id', 'mode', 'explicit']

# Drop them and make a copy
df_cleaned = df.drop(columns=drop_columns).dropna().copy()

# Confirm cleanup
print(list(df_cleaned.columns))

# Filtering only metal genres
print("BEFORE GENRE FILTERING")
print(f"Rows: {df_cleaned.shape[0]:,}")
print(f"Columns: {df_cleaned.shape[1]}")
print("Unique genres:", df_cleaned['track_genre'].nunique())

metal_genres = ["black-metal", "death-metal", "heavy-metal", "metal", "metalcore"]
df_cleaned = df_cleaned[df_cleaned['track_genre'].isin(metal_genres)].copy()

print("\nAFTER GENRE FILTERING")
print(f"Rows: {df_cleaned.shape[0]:,}")
print(f"Columns: {df_cleaned.shape[1]}")
print("Genres remaining:", df_cleaned['track_genre'].unique())

"""Exploratory Data Analysis + Business Questions: Explore the target variable valence and other audio features"""

# Target variable distribution (valence)
plt.figure(figsize=(12, 4))

# Histogram
plt.subplot(1, 2, 1)
plt.hist(df_cleaned['valence'], bins=50, edgecolor='black', alpha=0.7)
plt.title('Distribution of Song Valence')
plt.xlabel('Valence (0 = Sad, 1 = Happy)')
plt.ylabel('Frequency')

# Box plot
plt.subplot(1, 2, 2)
plt.boxplot(df_cleaned['valence'])
plt.title('Box Plot of Valence')
plt.ylabel('Valence Score')

plt.tight_layout()
plt.show()

# Summary stats
print("Valence Statistics:")
print(f"Mean: {df_cleaned['valence'].mean():.2f}")
print(f"Median: {df_cleaned['valence'].median():.2f}")
print(f"Standard Deviation: {df_cleaned['valence'].std():.2f}")

print("Visual Interpretations:")
print("1. Most songs in the dataset have low valence scores (0.0–0.3), indicating that they are generally darker or sadder in mood, which aligns with the characteristics of metal subgenres.")
print("2. The valence distribution is right-skewed, with a mean of 0.31 and median of 0.28, suggesting that high-valence (happier) songs are less common.")
print("3. The box plot reveals several outliers with high valence scores (> 0.8), which may represent genre crossovers or atypical emotional tones within the dataset.")

# Select only numeric columns for correlation
numeric_df = df_cleaned.select_dtypes(include=['number', 'bool'])

# Correlation matrix
plt.figure(figsize=(14, 12))
correlation_matrix = numeric_df.corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=0.5, fmt=".2f")

plt.title('Correlation Matrix of Spotify Audio Features')
plt.show()

# Show correlations with valence
print("Correlations with Valence (sorted by absolute value):")
valence_correlations = correlation_matrix['valence'].drop('valence').sort_values(key=abs, ascending=False)
print(valence_correlations)

#Visual Interpretations
print("Visual Interpretations:")
print("1. Valence is most positively correlated with danceability (r = 0.44), suggesting that happier songs in this dataset tend to be more danceable.")
print("2. Valence is negatively correlated with instrumentalness (r = -0.29), meaning more emotional or happy songs are less likely to be purely instrumental.")
print("3. Valence also has a moderate negative correlation with speechiness (r = -0.26) and duration (r = -0.25), indicating that sadder songs tend to have more spoken words and be slightly longer.")

# Scatter plots of most correlated features with valence
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('Relationships Between Key Features and Valence')

# Get top 4 most correlated features
top_features = valence_correlations.head(4).index

for i, feature in enumerate(top_features):
    row = i // 2
    col = i % 2

    axes[row, col].scatter(df_cleaned[feature], df_cleaned['valence'], alpha=0.5)
    axes[row, col].set_xlabel(feature)
    axes[row, col].set_ylabel('Valence')
    axes[row, col].set_title(f'{feature} vs Valence (r={valence_correlations[feature]:.3f})')

    # Add trend line
    z = np.polyfit(df_cleaned[feature], df_cleaned['valence'], 1)
    p = np.poly1d(z)
    axes[row, col].plot(df_cleaned[feature], p(df_cleaned[feature]), "r--", alpha=0.8)

plt.tight_layout()
plt.show()

#Visual Interpretations
print("Visual Interpretations:")
print("1. There is a moderate positive relationship between danceability and valence (r = 0.437), indicating that songs which are more danceable also tend to be more emotionally positive.")
print("2. Instrumentalness shows a weak to moderate negative correlation with valence (r = -0.289), suggesting that tracks without vocals are generally perceived as less happy.")
print("3. Speechiness and duration both have negative correlations with valence (r = -0.263 and r = -0.251), meaning that songs with more spoken words or longer durations tend to have lower emotional positivity.")

"""Feature Engineering - I created a new column called duration_min to convert the song duration from milliseconds to minutes.

"""

# Convert duration from milliseconds to minutes directly in df_cleaned
df_cleaned['duration_min'] = (df_cleaned['duration_ms'] / 60000).round(2)

# View the result
df_cleaned[['duration_ms', 'duration_min']].head()

# View the result
df_cleaned.head()

"""One-Hot Encoding"""

# Creating the tempo column into 4 labelled ranges for one-hot encoding
print("Before one-hot encoding:")
print(f"Shape: {df_cleaned.shape}")
print(f"Columns: {list(df_cleaned.columns)}")

df_cleaned['tempo_category'] = pd.cut(
    df_cleaned['tempo'],
    bins=[0, 75, 120, 175, float('inf')],
    labels=['Slow', 'Moderate', 'Fast', 'Extremely Fast']
)

print("New categorical features created:")
print("\nTempo Distribution:")
print(df_cleaned['tempo_category'].value_counts())

# One-hot encode tempo_category
df_cleaned = pd.get_dummies(df_cleaned, columns=['tempo_category'], drop_first=True)

print("\nAfter one-hot encoding:")
print(f"Shape: {df_cleaned.shape}")
print(f"Columns: {list(df_cleaned.columns)}")
df_cleaned.head()

# One-hot encoding for metal subgenres
print("BEFORE ONE-HOT ENCODING GENRE")
print(f"Shape: {df_cleaned.shape}")
print("Unique genres:", df_cleaned['track_genre'].unique())
print(f"Columns: {list(df_cleaned.columns)}")

# One-hot encode
df_cleaned = pd.get_dummies(df_cleaned, columns=['track_genre'], drop_first=True)

# After encoding
print("\nAFTER ONE-HOT ENCODING GENRE (as True/False)")
print(f"Shape: {df_cleaned.shape}")
print(f"Columns: {list(df_cleaned.columns)}")

# One-hot encoding for key
print("Before one-hot encoding for 'key':")
print(f"Shape: {df_cleaned.shape}")
print(f"Columns: {list(df_cleaned.columns)}")

key_labels = {
    0: 'key_scale_C',
    1: 'key_scale_C#',
    2: 'key_scale_D',
    3: 'key_scale_D#',
    4: 'key_scale_E',
    5: 'key_scale_F',
    6: 'key_scale_F#',
    7: 'key_scale_G',
    8: 'key_scale_G#',
    9: 'key_scale_A',
    10: 'key_scale_A#',
    11: 'key_scale_B'
}

df_cleaned['key_category'] = df_cleaned['key'].map(key_labels)

# One-hot encode the mapped key categories
df_cleaned = pd.get_dummies(df_cleaned, columns=['key_category'], drop_first=True)

print("\nAfter one-hot encoding for 'key':")
print(f"Shape: {df_cleaned.shape}")
print(f"Columns: {list(df_cleaned.columns)}")
df_cleaned.head()

print('\nAfter all one-hot encoding:')
print(f"Shape: {df_cleaned.shape}")

df_cleaned.head()

print('\n AFter one-hot encoding:')
print(f"Shape: {df_cleaned.shape}")

df_cleaned.to_csv("df_cleaned.csv", index=False)
from google.colab import files
files.download("df_cleaned.csv")

"""Business Question #1: Does track duration affect emotional valence?"""

# Business Question: Duration vs Valence

# Convert duration from milliseconds to minutes
df_cleaned['duration_min'] = (df_cleaned['duration_ms'] / 60000).round(2)

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_cleaned, x='duration_min', y='valence', alpha=0.5)
sns.regplot(data=df_cleaned, x='duration_min', y='valence', scatter=False, color='red')
plt.title('Duration vs Valence')
plt.xlabel('Duration (minutes)')
plt.ylabel('Valence (Positivity)')
plt.tight_layout()
plt.show()

#Visual Interpretations
print("Visual Interpretations:")
print("1. There is a clear negative relationship between song duration and valence, indicating that longer songs tend to be less emotionally positive.")
print("2. Most songs in the dataset fall between 2 to 6 minutes, where valence scores vary but still trend downward slightly.")
print("3. Extremely long tracks (over 10 minutes) are rare and often associated with very low valence, possibly reflecting more somber or experimental compositions in metal genres.")

"""Business Question #2: Which tempo range is most associated with emotional positivity?"""

# Recreate tempo_category for plotting (if needed)
df_cleaned['tempo_category_plot'] = pd.cut(
    df_cleaned['tempo'],
    bins=[0, 75, 120, 175, float('inf')],
    labels=['Slow', 'Moderate', 'Fast', 'Extremely Fast']
)

# Boxplot of valence by tempo category
df_cleaned.boxplot(column='valence', by='tempo_category_plot', grid=False)
plt.title('Valence by Tempo Range')
plt.suptitle('')
plt.xlabel('Tempo Range')
plt.ylabel('Valence')
plt.show()

#Visual Interpretations
print("Visual Interpretations:")
print("1. Valence tends to slightly increase with tempo, as faster and extremely fast songs show higher median valence compared to slow or moderate tracks.")
print("2. Despite the slight upward trend, the spread of valence is wide across all tempo ranges, suggesting that tempo alone isn't a strong predictor of emotional positivity.")
print("3. Each tempo category contains high-valence outliers, indicating that even slow songs can be perceived as emotionally positive in some cases.")

"""Business Question #3: Which metal subgenre is most emotionally positive?"""

# Recreate track_genre column for plotting only
df_cleaned['genre_for_plot'] = df['track_genre']

# Filter it to only metal genres again
metal_genres = ["black-metal", "death-metal", "heavy-metal", "metal", "metalcore"]
df_cleaned = df_cleaned[df_cleaned['genre_for_plot'].isin(metal_genres)].copy()

# Boxplot of valence by genre
df_cleaned.boxplot(column='valence', by='genre_for_plot', grid=False)
plt.title('Valence by Metal Subgenre')
plt.suptitle('')
plt.xlabel('Metal Subgenre')
plt.ylabel('Valence')
plt.show()

#Visual Interpretations
print("Visual Interpretations:")
print("1. Black-metal and death-metal songs have the lowest median valence scores, reflecting their typically darker and more aggressive emotional tone.")
print("2. Heavy-metal, metalcore, and especially general 'metal' tracks show higher median valence, indicating a wider emotional range or possible crossover appeal.")
print("3. All subgenres display a broad spread of valence, but black-metal and death-metal are especially skewed toward lower emotional positivity with many low-valence outliers.")

"""Business Question 4: What musical keys are most associated with emotional valence?"""

# Map key integers to key names again
key_labels = {
    0: 'key_scale_C', 1: 'key_scale_C#', 2: 'key_scale_D', 3: 'key_scale_D#',
    4: 'key_scale_E', 5: 'key_scale_F', 6: 'key_scale_F#', 7: 'key_scale_G',
    8: 'key_scale_G#', 9: 'key_scale_A', 10: 'key_scale_A#', 11: 'key_scale_B'
}
df_cleaned['key_label'] = df_cleaned['key'].map(key_labels)

df_cleaned.groupby('key_label')['valence'].mean().plot(kind='bar')
plt.title('Average Valence by Musical Key')
plt.xlabel('Key')
plt.ylabel('Valence')
plt.tight_layout()
plt.show()

#Visual Interpretations
print("Visual Interpretations:")
print("1. Average valence remains relatively consistent across all musical keys, suggesting that key alone does not strongly influence emotional positivity in this dataset.")
print("2. Key_scale_E has the lowest average valence, while key_scale_G# has the highest, though the difference is minimal.")
print("3. This uniformity implies that other musical features, such as tempo or danceability, likely have a greater impact on perceived emotional tone than key signature.")

"""Train Test"""

# Define target and features
y = df_cleaned['valence']
X = df_cleaned.drop(['valence', 'artists', 'track_name', 'album_name'], axis=1)

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"\nFeature names: {list(X.columns)}")

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\nTraining set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")
print(f"Training target shape: {y_train.shape}")
print(f"Testing target shape: {y_test.shape}")

"""Feature Scaling (Plus dropping non-numeric columns)"""

# Show original stats before scaling
print("Before scaling - Training set statistics:")
print(X_train.describe())

# Drop non-numeric columns that weren't one-hot encoded
X_train = X_train.drop(columns=['tempo_category_plot'], errors='ignore')
X_test = X_test.drop(columns=['tempo_category_plot'], errors='ignore')

# Drop plotting-only string columns before scaling
X_train = X_train.drop(columns=['genre_for_plot'], errors='ignore')
X_test = X_test.drop(columns=['genre_for_plot'], errors='ignore')

X_train = X_train.drop(columns=['key_label'], errors='ignore')
X_test = X_test.drop(columns=['key_label'], errors='ignore')

# Initialize scaler
scaler = StandardScaler()

# Fit and transform training set
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrames
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

# Show stats after scaling
print("\nAfter scaling - Training set statistics:")
print(X_train_scaled.describe())

# Compare means and stds before/after
print("\nScaling effect comparison:")
scaling_comparison = pd.DataFrame({
    'Original_Mean': X_train.mean(),
    'Original_Std': X_train.std(),
    'Scaled_Mean': X_train_scaled.mean(),
    'Scaled_Std': X_train_scaled.std()
})
print(scaling_comparison.head(8))

"""Multiple-Linear Regression"""

# Drop duration_ms to avoid duplicate with duration_min
X_train_scaled = X_train_scaled.drop(columns=['duration_ms'])
X_test_scaled = X_test_scaled.drop(columns=['duration_ms'])

# Train model
multiple_model = LinearRegression()
multiple_model.fit(X_train_scaled, y_train)

# Predictions
y_pred_multiple_train = multiple_model.predict(X_train_scaled)
y_pred_multiple_test = multiple_model.predict(X_test_scaled)

print("Multiple Linear Regression Results:")
print(f"Intercept: {multiple_model.intercept_:.4f}")

# Coefficients
coefficients_df = pd.DataFrame({
    'Feature': X_train_scaled.columns,
    'Coefficient': multiple_model.coef_
})
coefficients_df['Abs_Coefficient'] = abs(coefficients_df['Coefficient'])
coefficients_df = coefficients_df.sort_values('Abs_Coefficient', ascending=False)

print(coefficients_df)

"""Business Question #5: What audio features most influence emotional valence?"""

# Visualize coefficient importance for valence
plt.figure(figsize=(12, 8))
plt.barh(coefficients_df['Feature'], coefficients_df['Coefficient'])
plt.xlabel('Coefficient Value')
plt.title('Feature Coefficients in Multiple Linear Regression (Valence Prediction)')
plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()

#Visual Interpretations
print("Visual Interpretations:")
print("1. Danceability, track genre 'metal', and 'heavy-metal' have the strongest positive coefficients, suggesting they contribute the most to predicting higher valence scores.")
print("2. Speechiness, duration, and death-metal are among the most negatively weighted features, indicating they are associated with lower emotional positivity.")
print("3. Musical key and time signature show very small coefficients, reinforcing that they have minimal impact on valence prediction in this model.")

# Calculate performance metrics for multiple linear regression model
def calculate_metrics(y_true, y_pred, model_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f"{model_name} Valence Prediction Metrics:")
    print(f"  Mean Squared Error (MSE): {mse:.4f}")
    print(f"  Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"  Mean Absolute Error (MAE): {mae:.4f}")
    print(f"  R-squared (R²): {r2:.4f}")
    print(f"  Explained Variance: {r2 * 100:.2f}%")
    print()

    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2}

# Evaluate on training set
print("TRAINING SET VALENCE PERFORMANCE")
multiple_train_metrics = calculate_metrics(y_train, y_pred_multiple_train, "Multiple Linear Regression")

# Evaluate on testing set
print("TESTING SET VALENCE PERFORMANCE")
multiple_test_metrics = calculate_metrics(y_test, y_pred_multiple_test, "Multiple Linear Regression")

# Compare performance metrics
metrics_comparison = pd.DataFrame({
    'Train': [multiple_train_metrics['R2'], multiple_train_metrics['RMSE'], multiple_train_metrics['MAE']],
    'Test': [multiple_test_metrics['R2'], multiple_test_metrics['RMSE'], multiple_test_metrics['MAE']]
}, index=['R²', 'RMSE', 'MAE'])

print("Valence Model Performance Comparison:")
print(metrics_comparison)

# Visualize comparison
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# R²
axes[0].bar(['Train', 'Test'], [multiple_train_metrics['R2'], multiple_test_metrics['R2']], color=['lightgreen', 'gold'])
axes[0].set_title('R² Score (Valence)')
axes[0].set_ylabel('Score')
axes[0].set_ylim(0, 1)

# RMSE
axes[1].bar(['Train', 'Test'], [multiple_train_metrics['RMSE'], multiple_test_metrics['RMSE']], color=['lightgreen', 'gold'])
axes[1].set_title('RMSE (Valence)')
axes[1].set_ylabel('Error')

# MAE
axes[2].bar(['Train', 'Test'], [multiple_train_metrics['MAE'], multiple_test_metrics['MAE']], color=['lightgreen', 'gold'])
axes[2].set_title('MAE (Valence)')
axes[2].set_ylabel('Error')

plt.tight_layout()
plt.show()

#Visual Interpretation
print("Visual Interpretation")
print("1. The R² score is relatively low for both training (0.35) and testing (0.38), indicating that the model explains only a modest portion of variance in valence.")
print("2. RMSE and MAE values are nearly identical between the training and test sets, which suggests the model generalizes fairly well and is not overfitting.")
print("3. While the model performs consistently, the overall predictive power is limited, meaning that valence is likely influenced by non-linear relationships or unobserved features.")

"""Additional Regression Models for Valence Prediction"""

# Import libraries
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb

# Ridge Regression
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train_scaled, y_train)

# Predict
y_pred_ridge_train = ridge_model.predict(X_train_scaled)
y_pred_ridge_test = ridge_model.predict(X_test_scaled)

# Evaluate
print("RIDGE REGRESSION")
ridge_train_metrics = calculate_metrics(y_train, y_pred_ridge_train, "Ridge Regression (Train)")
ridge_test_metrics = calculate_metrics(y_test, y_pred_ridge_test, "Ridge Regression (Test)")

# Lasso Regression
lasso_model = Lasso(alpha=0.01)
lasso_model.fit(X_train_scaled, y_train)

# Predict
y_pred_lasso_train = lasso_model.predict(X_train_scaled)
y_pred_lasso_test = lasso_model.predict(X_test_scaled)

# Evaluate
print("LASSO REGRESSION")
lasso_train_metrics = calculate_metrics(y_train, y_pred_lasso_train, "Lasso Regression (Train)")
lasso_test_metrics = calculate_metrics(y_test, y_pred_lasso_test, "Lasso Regression (Test)")

from sklearn.ensemble import RandomForestRegressor

# Random Forest with max_depth only
rf_tuned = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)

# Fit model
rf_tuned.fit(X_train_scaled, y_train)

# Predict
y_pred_rf_tuned_train = rf_tuned.predict(X_train_scaled)
y_pred_rf_tuned_test = rf_tuned.predict(X_test_scaled)

# Evaluate
print("RANDOM FOREST (max_depth=6)")
rf_tuned_train_metrics = calculate_metrics(y_train, y_pred_rf_tuned_train, "Random Forest (Train)")
rf_tuned_test_metrics = calculate_metrics(y_test, y_pred_rf_tuned_test, "Random Forest (Test)")

# Gradient Boosting
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gb_model.fit(X_train_scaled, y_train)

# Predict
y_pred_gb_train = gb_model.predict(X_train_scaled)
y_pred_gb_test = gb_model.predict(X_test_scaled)

# Evaluate
print("GRADIENT BOOSTING REGRESSION")
gb_train_metrics = calculate_metrics(y_train, y_pred_gb_train, "Gradient Boosting (Train)")
gb_test_metrics = calculate_metrics(y_test, y_pred_gb_test, "Gradient Boosting (Test)")

# XGBoost
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
xgb_model.fit(X_train_scaled, y_train)

# Predict
y_pred_xgb_train = xgb_model.predict(X_train_scaled)
y_pred_xgb_test = xgb_model.predict(X_test_scaled)

# Evaluate
print("XGBOOST REGRESSION")
xgb_train_metrics = calculate_metrics(y_train, y_pred_xgb_train, "XGBoost (Train)")
xgb_test_metrics = calculate_metrics(y_test, y_pred_xgb_test, "XGBoost (Test)")

"""Cross-Validation and Performance Evaluation for all regression models"""

# Import Required Libraries and Define RMSE Scorer

from sklearn.model_selection import cross_validate, KFold
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Custom RMSE scorer function
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

rmse_scorer = make_scorer(rmse, greater_is_better=False)

# Setup Cross-Validation, Define Models, Metrics, and Run Evaluation

# Drop unused or non-numeric columns
X = X.drop(columns=['genre_for_plot', 'key_label', 'tempo_category'], errors='ignore')

# One-hot encode any remaining object columns (just in case)
X = pd.get_dummies(X, drop_first=True)

# Confirm all columns are numeric
print("Non-numeric columns in X:", X.select_dtypes(include='object').columns.tolist())
print("Categorical columns in X:", X.dtypes[X.dtypes == 'category'])

# K-Fold Cross-Validation setup
cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Define models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, verbosity=0)
}

# Define scoring metrics for CV
scoring = {
    'r2': 'r2',
    'mae': 'neg_mean_absolute_error',
    'rmse': rmse_scorer  # Custom RMSE scorer defined earlier
}

# Run cross-validation on each model and store results
cv_results = []

for name, model in models.items():
    print(f"Running CV for: {name}")
    scores = cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=False)

    cv_results.append({
        'Model': name,
        'R² (Mean)': np.mean(scores['test_r2']),
        'MAE (Mean)': -np.mean(scores['test_mae']),
        'RMSE (Mean)': -np.mean(scores['test_rmse'])
    })

# Display results
cv_df = pd.DataFrame(cv_results)
cv_df = cv_df.sort_values(by='R² (Mean)', ascending=False)
print("\nCross-Validation Results")
print(cv_df.round(4))

"""Model Comparison with clear justification for final model choice"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Model names
model_names = ['Linear', 'Ridge', 'Lasso', 'Random Forest', 'Gradient Boosting', 'XGBoost']

# Collect Train and Test metrics from each model
r2_train = [
    multiple_train_metrics['R2'],
    ridge_train_metrics['R2'],
    lasso_train_metrics['R2'],
    rf_tuned_train_metrics['R2'],
    gb_train_metrics['R2'],
    xgb_train_metrics['R2']
]

r2_test = [
    multiple_test_metrics['R2'],
    ridge_test_metrics['R2'],
    lasso_test_metrics['R2'],
    rf_tuned_test_metrics['R2'],
    gb_test_metrics['R2'],
    xgb_test_metrics['R2']
]

rmse_train = [
    multiple_train_metrics['RMSE'],
    ridge_train_metrics['RMSE'],
    lasso_train_metrics['RMSE'],
    rf_tuned_train_metrics['RMSE'],
    gb_train_metrics['RMSE'],
    xgb_train_metrics['RMSE']
]

rmse_test = [
    multiple_test_metrics['RMSE'],
    ridge_test_metrics['RMSE'],
    lasso_test_metrics['RMSE'],
    rf_tuned_test_metrics['RMSE'],
    gb_test_metrics['RMSE'],
    xgb_test_metrics['RMSE']
]

mae_train = [
    multiple_train_metrics['MAE'],
    ridge_train_metrics['MAE'],
    lasso_train_metrics['MAE'],
    rf_tuned_train_metrics['MAE'],
    gb_train_metrics['MAE'],
    xgb_train_metrics['MAE']
]

mae_test = [
    multiple_test_metrics['MAE'],
    ridge_test_metrics['MAE'],
    lasso_test_metrics['MAE'],
    rf_tuned_test_metrics['MAE'],
    gb_test_metrics['MAE'],
    xgb_test_metrics['MAE']
]

# Create summary DataFrame
summary_df = pd.DataFrame({
    "Model": model_names,
    "R² (Train)": r2_train,
    "R² (Test)": r2_test,
    "RMSE (Train)": rmse_train,
    "RMSE (Test)": rmse_test,
    "MAE (Train)": mae_train,
    "MAE (Test)": mae_test
})

# Print table
print("\n Train vs Test Model Performance Summary")
print(summary_df.set_index("Model").round(4).to_string())

# Visualization
x = np.arange(len(model_names))
width = 0.35

fig, axs = plt.subplots(1, 3, figsize=(18, 5))

# R² Score
axs[0].bar(x - width/2, r2_train, width, label='Train', color='lightgreen')
axs[0].bar(x + width/2, r2_test, width, label='Test', color='gold')
axs[0].set_title('R² Score')
axs[0].set_xticks(x)
axs[0].set_xticklabels(model_names, rotation=45)
axs[0].set_ylabel('Score')
axs[0].set_ylim(0, 1)
axs[0].legend()

# RMSE
axs[1].bar(x - width/2, rmse_train, width, label='Train', color='lightgreen')
axs[1].bar(x + width/2, rmse_test, width, label='Test', color='gold')
axs[1].set_title('RMSE')
axs[1].set_xticks(x)
axs[1].set_xticklabels(model_names, rotation=45)
axs[1].set_ylabel('Error')
axs[1].legend()

# MAE
axs[2].bar(x - width/2, mae_train, width, label='Train', color='lightgreen')
axs[2].bar(x + width/2, mae_test, width, label='Test', color='gold')
axs[2].set_title('MAE')
axs[2].set_xticks(x)
axs[2].set_xticklabels(model_names, rotation=45)
axs[2].set_ylabel('Error')
axs[2].legend()

plt.suptitle("Train vs Test Model Performance Comparison", fontsize=16)
plt.tight_layout()
plt.show()

#Inerpretations
print("\n Model Performance Interpretation")
print("1. XGBoost and Gradient Boosting achieved the highest R² scores among all models, with XGBoost slightly outperforming on both train (0.5574) and test (0.5105) sets, indicating strong predictive power.")
print("2. Linear, Ridge, and Lasso regressions had significantly lower R² and higher RMSE/MAE, suggesting they struggled to capture the complexity of the data, likely due to their inability to model non-linear relationships.")
print("3. The performance gap between training and testing is small across all models, indicating minimal overfitting and good generalization.")
print("Based on these results, the best regression model to use for predicting valence is XGBoost, as it consistently delivered the best accuracy and lowest errors on both the training and test datasets.")

#Disclaimer
print("Disclaimer: Due to many external factors influencing this project, the R-squared value is only about 51%")

# Save the final XGBoost model
import joblib
joblib.dump(xgb_model, 'xgb_model.pkl')

# Save the fitted StandardScaler
joblib.dump(scaler, 'scaler.pkl')

# Download both files
from google.colab import files
files.download('xgb_model.pkl')
files.download('scaler.pkl')
